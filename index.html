<!DOCTYPE html>
<html lang="en">
  <head>
      <meta charset="UTF-8">
      <link rel="stylesheet" href="main.css">
      <link rel="icon" type="image/x-icon" href="assets/digit.png">
      <title>advnav</title>
  </head>
  <body>


    <div id="title_slide">
        <div class="title_left">
            <h1>Iterative Adversarial Learning with Chaser Agents <br> for Time-efficient Crowd-aware Navigation</h1>
        

            <div class="box alt">
              <div class="row uniform">
                <div class="2u">
                  <a href="https://www.weizhu996.com/">                                
                    <span class="image fit"><img src="assets/author_photo/weizhu.png" alt="Wei Zhu" /></span>Wei Zhu<sup>1</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://abirathr.github.io/AbirathRaju.github.io/">         
                    <span class="image fit"><img src="assets/author_photo/abirath.jpg" alt="Abirath Raju" /></span>Abirath Raju<sup>1</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://scholar.google.com/citations?user=41GA5O4AAAAJ&hl=en">     
                    <span class="image fit"><img src="assets/author_photo/aziz_shamsah.png" alt="Abdulaziz Shamsah" /></span>Abdulaziz Shamsah<sup>2</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://sites.google.com/site/anqiwuresearch/">                 
                    <span class="image fit"><img src="assets/author_photo/anqi.jpg" alt="Anqi Wu" /></span>Anqi Wu<sup>1</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://scholar.google.com/citations?user=-JPZ21IAAAAJ&hl=en">                 
                    <span class="image fit"><img src="assets/author_photo/seth.png" alt="Seth Hutchinson" /></span>Seth Hutchinson<sup>3</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://research.gatech.edu/people/ye-zhao">   
                    <span class="image fit"><img src="assets/author_photo/ye_zhao.jpg" alt="Ye Zhao" /></span>Ye Zhao<sup>1</sup>
                  </a>
                </div>
              </div>
          </div>

            <div class="gatech">
                <p><sup>1</sup>Georgia Institute of Technology, <sup>2</sup>Kuwait University, <sup>3</sup>Northeastern University</p>
            </div>
            <div class="button-container">
                <a href="http://arxiv.org/abs/2503.12538" class="button">Paper</a>
                <a href="https://youtu.be/fNNL56sTSjY?si=sLgWn5P6o4MndMzl" class="button">Video</a>
                <a href="https://github.com/GTLIDAR/emobipednav" class="button">Code</a>
            </div>

            <br>

            <div id="abstract" class="grid-container">
                <p>
                  This paper addresses the challenge of safe and efficient crowd navigation for autonomous robots in dynamic environments. 
									Existing methods struggle in scenarios with unpredictable or obstructive pedestrian behaviors. These limitations raise serious 
									safety and efficiency concerns in real-world deployments. To improve navigation robustness and efficiency, we propose an adversarial 
									deep reinforcement learning (DRL) framework that simulates competitive pedestrian behaviors through a chaser agent. In addition, we further 
									introduce an ensemble agent that dynamically selects policies based on real-time observations, enhancing generalization across diverse scenarios.
									Extensive simulation results demonstrate enhanced navigation performance in terms of success rate and navigation efficiency when using our framework.
									Additionally, the ensemble agent further improves stability and overall generalizability across various environments. Real-world experiments 
									validate the approach and demonstrate the potential for sim-to-real transfer. 
                </p>
            </div>
        </div>
    </div>
    <hr class="rounded">
    
    <div id="overview">

        <h1>EmoBipedNav</h1>

        <p>
          Our framework begins by obtaining estimated facial emotions using pre-trained CNN models. Simultaneously, we transform raw LiDAR 
          scans into sequential pie-shape lidar grid maps (LGMs). These grid maps are converted into stacked pixel images which are further 
          processed through an encoder constructed using convolutional neural networks (CNNs) to extract socially interactive and emotionally 
          aware features. The resulting latent features are concatenated with the robot's last command and target position, which are fed 
          into an actor-critic deep reinforcement learning (DRL) structure implemented with multi-layer perceptrons (MLPs). The action output 
          from the actor network is derived from the reduce-order model (ROM) and practically applied to a bipedal robot with full-body 
          dynamics and constraints. The torso position and yaw angle obtained from Digit correspond to the ego-agent state. We use the angular 
          momentum linear inverted pendulum planner (ALIP) and a passivity full-body controller with ankle actuation to track the desirable 
          ROM trajectory.
        </p>

        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/uOsFafqcxEs?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <h1>Simulation benchmark in MuJoCo</h1>
        <p>
          Initially, we train and test our navigation policy in a physics-based simulator MuJoCo, using a full-body bipedal robot and a full-order 
          dynamics. Simulatioin comparisons demonstrate the effectiveness of our social navigation pipeline for bipedal robots.
        </p>

        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/DsfEIVZ708A?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <h1>Simulation in complex scenario</h1>
        <p>
          We deploy our policy into a more complex simulation scenarios, further indicating that our navigation policy enables the bipedal 
          robot to adapt to complex and interactive scenarios integrated with pedestrian emotions.
        </p>

        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/IpVLqscL89o?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <h1>Sim-to-real</h1>
        <p>
          We transfer our simulated navigation policy into real-world scenarios with diverse pedestrian motion patterns. The robot should avoid 
          static obstacles while interacting with pedestrians. We include several representative motion patterns. For example, pedestrians 
          cross in front of the robot, walk and suddenly stop in front of the robot, group together to cross in front of the robot, and randomly 
          interact with the robot and other pedestrians. Furthermore, we randomly set pedestrian emotions. Accordingly, our social navigation 
          policy can well adapt to its own behaviors to avoid collisions and reduce intrusions into discomfort zones of pedestrians, indicating 
          the potential of sim-to-real transfer of our navigation policy. In addition, we integrate robot localization, pedestrian detection, 
          and emotion recognition into our pipeline to further validate the practicality of our emotion-aware social navigation. These modules 
          are achieved by an on-board stereo camera, and corresponding SLAM and perception algorithms. Pedestriansâ€™ states can change between 
          static and moving, and their emotions also alter between happy, neutral, and negative. To investigate further, we increase the 
          environment complexities. For example, pedestrians group together and keep static to block the robot. Moreover, we increase the 
          pedestrian density to make the scenario more crowded. 
        </p>
        
        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/KnCzS2p8oLc?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <h1>Outdoor experiments</h1>
        <p>
          Finally, we deploy our navigation policy into more realistic environments, like the campus after classes. Pedestrians are more 
          crowded, and interactions are more complicated. The bipedal robot can still achieve the emotion-aware social navigation task.
        </p>

        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/QGdr_MnXXw0?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <h1>Outdoor experiment in a dense crowd</h1>
        <p>
          We implemented our policy in a more dense environment, with pedestrian detection and emotion recoginition shown in the top video.
        </p>
        
        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/r_3txTwhS3w?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <script>
          document.addEventListener("DOMContentLoaded", function() {
              document.querySelectorAll(".autoplay-video").forEach(function(iframe) {
                  iframe.src = iframe.getAttribute("data-src"); // Dynamically set the `src`
              });
          });
          </script>
       
        <h1>BibTeX</h1>
         <p class="bibtex">
            @article{zhu2025emobipednav,<br>
            &nbsp;&nbsp;title={EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with Deep Reinforcement Learning},<br>
            &nbsp;&nbsp;author={Wei Zhu, Abirath Raju, Abdulaziz Shamsah, Anqi Wu, Seth Hutchinson, and Ye Zhao},<br>
            &nbsp;&nbsp;journal={arXiv preprint arXiv:2503.12538}<br>
            &nbsp;&nbsp;year={2025},<br>
            }
        </p>

      
       

        <div class="footer">
          <p>This website was developed based on <a href="https://github.com/learning-humanoid-locomotion/learning-humanoid-locomotion.github.io" target="_blank">learning-humanoid-locomotion</a></p>
      </div>
      
    </div>
    <script type="text/javascript">
        /* https://stackoverflow.com/questions/3027707/how-to-change-the-playing-speed-of-videos-in-html5 */
        document.querySelector('video').defaultPlaybackRate = 1.0;
        document.querySelector('video').play();

        var videos =document.querySelectorAll('video');
        for (var i=0;i<1;i++)
        {
            videos[i].playbackRate = 1.0;
        }
    </script>
    <script>
        /* https://stackoverflow.com/questions/21163756/html5-and-javascript-to-play-videos-only-when-visible */
        var videos = document.getElementsByTagName("video");

        function checkScroll() {
            var fraction = 0.5; // Play when 70% of the player is visible.

            for(var i = 0; i < 1; i++) {  // only apply to the first video

                var video = videos[i];

                var x = video.offsetLeft, y = video.offsetTop, w = video.offsetWidth, h = video.offsetHeight, r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }
        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Function to check if the user is on a mobile device
            function isMobileDevice() {
                return /Mobi|Android/i.test(navigator.userAgent);
            }
            // If the user is on a mobile device, disable autoplay
            if (isMobileDevice()) {
                const videos = document.querySelectorAll('video');
                videos.forEach(video => {
                    video.autoplay = false;
                    video.controls = true;
                });
            }
        });
    </script>
    <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f"
            type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
            crossorigin="anonymous"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"
            type="text/javascript"></script>
  </body>

</html>
